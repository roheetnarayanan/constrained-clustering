{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2025b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "ENG_STOP = stopwords.words(\"ENGLISH\")\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from copkmeans.cop_kmeans import cop_kmeans\n",
    "from nltk.stem import *\n",
    "import gensim\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cdc31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_la = pd.read_pickle(\"LATIMES_df\")\n",
    "df_la[\"From\"] = \"LA Times\"\n",
    "df_fbis = pd.read_pickle(\"df_1000_docs\")\n",
    "df_fbis[\"From\"] = \"FBIS\"\n",
    "df_ft = pd.read_pickle(\"FT_df\")\n",
    "df_ft[\"From\"] = \"FT\"\n",
    "df_la = df_la.sample(n=500,axis=0,random_state=11)\n",
    "df_fbis = df_fbis.sample(n=500,axis=0,random_state=11)\n",
    "df_ft = df_ft.sample(n=500,axis=0,random_state=11)\n",
    "df = pd.concat([df_la,df_fbis,df_ft],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2198a054",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "20fbf1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Text\"]!=\"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1ee5cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"Raw_Df_130622\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b2a22838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1478"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.Text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cac6ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([str(token.lemma_) for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "def cleanhtml(raw_html):\n",
    "    CLEANR = re.compile('<.*?>') \n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    cleantext = cleantext.replace(\"/\",\" \").replace(\"(\",\"\").replace(\")\",\"\")\n",
    "    res = []\n",
    "    l_res = []\n",
    "    for word in cleantext.split(\" \"):\n",
    "        word = word.lower()\n",
    "        if word.isalpha() and word not in ENG_STOP:\n",
    "            res.append(word)\n",
    "    return res\n",
    "data = df.Text.to_list()\n",
    "cleaned_data = [cleanhtml(doc) for doc in data]\n",
    "cleaned_data = lemmatization(cleaned_data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8976911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cleaned_data,open(\"cleaned_data_130622_F.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c1e86700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(text,max_features):\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False,max_features=max_features)\n",
    "    bow = vectorizer.fit_transform(text)\n",
    "    text_d = bow.toarray()\n",
    "    temp = pd.DataFrame(text_d, columns = vectorizer.get_feature_names())\n",
    "    return temp\n",
    "\n",
    "def get_tfidf(text,max_features):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False,max_features=max_features)\n",
    "    bow = vectorizer.fit_transform(text)\n",
    "    text_d = bow.toarray()\n",
    "    temp = pd.DataFrame(text_d, columns = vectorizer.get_feature_names())\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c20608",
   "metadata": {},
   "outputs": [],
   "source": [
    "GoogleModel = gensim.models.KeyedVectors.load_word2vec_format(r\"\\GoogleNews-vectors-negative300.bin.gz\", binary=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "800c9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function which takes text input and returns one vector for each sentence\n",
    "def FunctionText2Vec(GoogleModel = gensim.models.KeyedVectors.load_word2vec_format(r\"\\GoogleNews-vectors-negative300.bin.gz\", binary=True,)inpTextData, cleaned_data,max_features):\n",
    "    CountVectData=get_bow(cleaned_data,max_features=max_features)\n",
    "    \n",
    "    # Creating empty dataframe to hold sentences\n",
    "    W2Vec_Data=pd.DataFrame()\n",
    "    WordsVocab=CountVectData.columns[:]\n",
    "    # Looping through each row for the data\n",
    "    for i in range(CountVectData.shape[0]):\n",
    "\n",
    "        # initiating a sentence with all zeros\n",
    "        Sentence = np.zeros(300)\n",
    "\n",
    "        # Looping thru each word in the sentence and if its present in \n",
    "        # the Word2Vec model then storing its vector\n",
    "        for word in WordsVocab[CountVectData.iloc[i,:]>=1]:\n",
    "            #print(word)\n",
    "            if word in GoogleModel.key_to_index.keys():    \n",
    "                Sentence=Sentence+GoogleModel[word]\n",
    "        # Appending the sentence to the dataframe\n",
    "        W2Vec_Data=W2Vec_Data.append(pd.DataFrame([Sentence]), ignore_index=True)\n",
    "    return W2Vec_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "971e7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function to convert all the text data to Word2Vec Vectors\n",
    "Data=FunctionText2Vec(df['Text'], cleaned_data,max_features=10000)\n",
    "# Checking the new representation for sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6d49649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.columns = [\"W2Vec_\"+str(i) for i in range(300)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d0e786e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf = get_tfidf(cleaned_data,max_features=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f589ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.concat([Data,data_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f145de00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kw_model = KeyBERT('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d9903b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohee\\anaconda3\\lib\\site-packages\\keybert\\_model.py:130: UserWarning: Although extracting keywords for multiple documents is faster than iterating over single documents, it requires significantly more memory to hold all word embeddings. Use this at your own discretion!\n",
      "  warnings.warn(\n",
      "1478it [01:33, 15.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Finding the key phrase for each document\n",
    "keywords = kw_model.extract_keywords(df[\"Text\"].to_list(), keyphrase_ngram_range=(3, 3), stop_words='english',\n",
    "                              use_maxsum=True, nr_candidates=30, top_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b87bcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the key phrase using Sentence Transformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "kw = keywords\n",
    "emb = []\n",
    "for k in kw:\n",
    "    emb.append(model.encode(k[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "149746c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_df = pd.DataFrame(emb, columns=[\"KP_\"+str(i+1) for i in range(len(emb[0]))])\n",
    "#kp_df_scaled = min_max_scaler.fit_transform(kp_df.values)\n",
    "#kp_df = pd.DataFrame(kp_df_scaled,columns=[\"KP_\"+str(i+1) for i in range(len(emb[0]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d9aa7cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.concat([Data, kp_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "13621612",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.to_pickle(\"Data130622_F\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
